# AbstractLLM Library Architecture Mindmap

## Overview
AbstractLLM is a unified interface library for interacting with multiple Large Language Model providers. It abstracts the differences between providers while maintaining access to provider-specific features.

## Core Architecture

### 1. Factory Pattern (`factory.py`)
```
create_llm(provider, **config) -> AbstractLLMInterface
â”œâ”€â”€ Provider validation & dependency checking
â”œâ”€â”€ Platform requirements validation (e.g., MLX on Apple Silicon)
â”œâ”€â”€ API key management (env vars, explicit)
â”œâ”€â”€ Registry-based provider resolution
â””â”€â”€ Fallback to hardcoded provider mapping
```

### 2. Abstract Interface (`interface.py`)
```
AbstractLLMInterface (ABC)
â”œâ”€â”€ generate(prompt, system_prompt, files, stream, tools, **kwargs)
â”œâ”€â”€ generate_async(...)
â”œâ”€â”€ get_capabilities() -> Dict[ModelCapability, Any]
â”œâ”€â”€ Configuration management via ConfigurationManager
â””â”€â”€ Provider-specific parameter handling
```

### 3. Provider Registry System (`providers/registry.py`)
```
Dynamic Provider Loading
â”œâ”€â”€ register_provider(name, module_path, class_name)
â”œâ”€â”€ get_provider_class(name) -> lazy import
â”œâ”€â”€ Platform-specific registration (MLX checks)
â””â”€â”€ Dependency validation before registration
```

### 4. Base Provider (`providers/base.py`)
```
BaseProvider(AbstractLLMInterface)
â”œâ”€â”€ Tool validation & processing
â”œâ”€â”€ Tool call extraction from responses
â”œâ”€â”€ Response processing & standardization
â””â”€â”€ Common functionality across providers
```

## Provider Implementations

### Supported Providers
- **OpenAI** (`openai.py`) - GPT models, vision support, tool calling
- **Anthropic** (`anthropic.py`) - Claude models, tool calling
- **Ollama** (`ollama.py`) - Local models, self-hosted
- **HuggingFace** (`huggingface.py`) - Open source models
- **MLX** (`mlx_provider.py` + `mlx_model_configs.py`) - Apple Silicon optimized models

### MLX Provider Architecture

The MLX provider uses a **two-file architecture** that's justified by its complexity:

```
MLX Provider Structure
â”œâ”€â”€ mlx_provider.py - Core provider logic
â”‚   â”œâ”€â”€ Model loading (text & vision models) âœ… COMPLETE
â”‚   â”œâ”€â”€ Generation (sync/async, streaming) âœ… COMPLETE
â”‚   â”œâ”€â”€ Apple Silicon platform checks âœ… COMPLETE
â”‚   â”œâ”€â”€ Vision model detection & processing âœ… COMPLETE
â”‚   â””â”€â”€ Tool support (basic via prompt engineering) âœ… COMPLETE
â””â”€â”€ mlx_model_configs.py - Model-specific configurations âœ… COMPLETE
    â”œâ”€â”€ ModelConfigFactory - Maps model names to configs âœ… COMPLETE
    â”œâ”€â”€ Base MLXModelConfig class âœ… COMPLETE
    â”œâ”€â”€ Model families: Llama, Qwen, Mistral, Phi, etc. âœ… COMPLETE
    â”œâ”€â”€ Vision models: PaliGemma, Qwen2VL, etc. âœ… COMPLETE
    â””â”€â”€ Format handlers: chat templates, tokens, generation params âœ… COMPLETE
```

**âœ… MLX Provider Status: COMPLETE AND THOROUGHLY TESTED**

The MLX provider now works with the **same simple pattern** as other providers and has been verified with comprehensive testing:

**Core Functionality:**
```python
# Simple usage - works exactly like Ollama!
provider = create_llm("mlx", model="mlx-community/Qwen3-30B-A3B-4bit")
response = provider.generate("Hello!")
```

**âœ… Verified Capabilities (all tested and working):**
- âœ… **Basic Generation**: Standard text generation
- âœ… **Streaming Generation**: Real-time token streaming  
- âœ… **Async Generation**: Asynchronous generation support
- âœ… **System Prompts**: System prompt handling (**FIXED**: No longer ignored)
- âœ… **Tool Calling**: Basic tool support via prompt engineering (**FIXED**: No infinite loop)
- âœ… **Vision Models**: Vision model detection and capabilities
- âœ… **File Handling**: File input processing

**ðŸ› BUGS FIXED:**
- âœ… **Infinite Loop in Tool Calling**: System prompts were being ignored, causing Session to loop indefinitely waiting for tool calls that never happened. **FIXED** by properly passing and formatting system prompts in `_generate_text()` and `_generate_text_stream()` methods.
- âœ… **System Prompt Ignored**: The `_generate_text` methods weren't using the system_prompt parameter. **FIXED** by formatting system and user prompts together using model-specific chat templates.

**âš ï¸ Known Limitations:**
- **Tool calling is "basic"**: Uses prompt engineering, not structured API like OpenAI/Anthropic
- **Response formatting**: May include chat template artifacts in responses
- **Tool execution**: Model shows "thinking" process rather than clean tool execution

### MLX-Specific Features
```
Capabilities
â”œâ”€â”€ Text-only models (via mlx-lm)
â”œâ”€â”€ Vision models (via mlx-vlm) 
â”œâ”€â”€ Streaming generation (stream_generate)
â”œâ”€â”€ Model-specific optimizations
â”œâ”€â”€ Apple Silicon platform validation
â”œâ”€â”€ Automatic model config detection
â””â”€â”€ Basic tool support (prompt engineering)
```

### Provider Features
```
Each Provider Implements:
â”œâ”€â”€ Model-specific configuration & defaults
â”œâ”€â”€ Provider-specific API format conversion
â”œâ”€â”€ Error handling & exception mapping
â”œâ”€â”€ Capability reporting (streaming, vision, tools)
â”œâ”€â”€ Tool calling format conversion
â””â”€â”€ Response standardization
```

## Session Management (`session.py`)

### Session Class
```
Session
â”œâ”€â”€ Conversation history management
â”œâ”€â”€ Provider switching mid-conversation
â”œâ”€â”€ Tool function management & execution
â”œâ”€â”€ Automatic tool calling workflows
â”œâ”€â”€ Save/load conversation state
â””â”€â”€ Metadata tracking
```

### Key Features
- **Provider Interchangeability**: Switch providers while maintaining context
- **Tool Execution**: Automatic tool calling with function registration
- **Streaming Support**: Real-time response generation
- **Persistence**: Save/restore conversation sessions

## Tool System (`tools/`)

### Architecture
```
Tool System
â”œâ”€â”€ Function-to-tool conversion (docstring parsing)
â”œâ”€â”€ Tool definition validation (JSON schema)
â”œâ”€â”€ Provider-specific format conversion
â”œâ”€â”€ Automatic tool execution in sessions
â””â”€â”€ Tool call result processing
```

### Workflow
1. Functions with type hints â†’ Tool definitions
2. Tool definitions â†’ Provider-specific format
3. LLM response â†’ Tool call extraction
4. Tool execution â†’ Results back to LLM

## Type System

### Core Types (`types.py`)
```
GenerateResponse
â”œâ”€â”€ content: Optional[str]
â”œâ”€â”€ raw_response: Any
â”œâ”€â”€ usage: Optional[Dict[str, int]]
â”œâ”€â”€ tool_calls: Optional[ToolCallRequest]
â””â”€â”€ has_tool_calls() -> bool

Message
â”œâ”€â”€ role: Union[str, MessageRole]
â”œâ”€â”€ content: str
â”œâ”€â”€ tool_results: Optional[List[Dict]]
â””â”€â”€ to_dict() / from_dict()
```

### Enums (`enums.py`)
```
ModelParameter - Configuration parameters
â”œâ”€â”€ Basic: temperature, max_tokens, model, api_key
â”œâ”€â”€ Advanced: top_p, frequency_penalty, seed
â”œâ”€â”€ Vision: image, images, image_detail
â””â”€â”€ Tools: tools, tool_choice

ModelCapability - Provider capabilities
â”œâ”€â”€ Basic: streaming, async, system_prompt
â”œâ”€â”€ Advanced: function_calling, vision, tool_use
â””â”€â”€ Specialized: fine_tuning, embeddings, json_mode

MessageRole - Conversation roles
â”œâ”€â”€ system, user, assistant, tool
```

## Configuration Management (`utils/config.py`)

### Features
- Parameter validation and type checking
- Default value management per provider
- Runtime configuration updates
- Provider-specific parameter mapping

## Media Support (`media/`)

### Vision Capabilities
- Image processing for vision models
- Multi-modal content handling
- File type detection and processing
- URL and local file support

## Key Design Principles

### 1. Provider Abstraction
- Consistent API across all providers
- Provider-specific features accessible via kwargs
- Graceful degradation for unsupported features

### 2. Lazy Loading
- Providers loaded on-demand
- Dependency checking before import
- Platform-specific provider registration

### 3. Extensibility
- Registry system for custom providers
- Tool system for function calling
- Configuration system for customization

### 4. Error Handling
- Standardized exceptions across providers
- Clear error messages with resolution hints
- Graceful fallbacks where possible

### 5. Developer Experience
- Type-safe parameters via enums
- Comprehensive logging and debugging
- Clear documentation and examples
- Optional dependencies with helpful error messages

## Usage Patterns

### Basic Usage
```python
# Simple generation
llm = create_llm("openai", api_key="...")
response = llm.generate("Hello!")

# Provider switching
openai_llm = create_llm("openai")
claude_llm = create_llm("anthropic")
```

### Session-based Usage
```python
# Stateful conversation
session = Session(system_prompt="...", provider=llm)
session.add_message("user", "Hello")
response = session.generate("How are you?")
```

### Tool Calling
```python
# Function registration
def get_weather(location: str) -> str:
    return f"Weather in {location}"

session = Session(tools=[get_weather])
response = session.generate_with_tools("What's the weather in Paris?")
```

## Dependencies & Installation

### Core Dependencies
- `pydantic` - Configuration and validation
- `typing-extensions` - Enhanced typing

### Provider Dependencies
- `openai` - OpenAI API
- `anthropic` - Anthropic API  
- `torch`, `transformers` - HuggingFace
- `mlx`, `mlx-lm` - Apple Silicon MLX
- `requests`, `aiohttp` - Ollama

### Optional Dependencies
- `docstring-parser`, `jsonschema` - Tool support
- `pillow` - Image processing

This architecture provides a clean abstraction layer while maintaining flexibility and extensibility for different LLM providers and use cases. 